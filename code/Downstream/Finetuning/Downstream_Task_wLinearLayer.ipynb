{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "import random\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch\n",
    "import os\n",
    "from torch.optim import *\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "from scipy import ndimage\n",
    "import pandas as pd\n",
    "from copy import deepcopy\n",
    "import random\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "random.seed(7)\n",
    "torch.cuda.manual_seed(7)\n",
    "torch.manual_seed(7)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "np.random.seed(7)\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "else:\n",
    "    device = torch.device('cpu')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_ROOT = '/beegfs/bva212/openmic-2018'\n",
    "OPENMIC = np.load(os.path.join(DATA_ROOT, 'openmic-2018.npz'))\n",
    "X, Y_true, Y_mask, sample_key = OPENMIC['X'], OPENMIC['Y_true'], OPENMIC['Y_mask'], OPENMIC['sample_key']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_samples = pd.read_csv(os.path.join(DATA_ROOT,'partitions/split01_train.csv'), names =['id']).to_numpy().squeeze()\n",
    "test_samples = pd.read_csv(os.path.join(DATA_ROOT,'partitions/split01_test.csv'), names =['id']).to_numpy().squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "len_data = len(train_samples)\n",
    "idx_train = np.random.choice(len_data, int(len_data*0.8), replace=False)\n",
    "remain_set = list(set(np.arange(len_data))-set(idx_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_idx = np.isin(sample_key, train_samples[remain_set])\n",
    "train_idx = np.isin(sample_key, train_samples[idx_train])\n",
    "test_idx = np.isin(sample_key, test_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ArrowOfTime(Dataset):\n",
    "\n",
    "    def __init__(self, root_dir, files, weights, label):\n",
    "        self.weights = weights\n",
    "        self.device = device\n",
    "        self.root_dir = root_dir\n",
    "        self.files = files\n",
    "        self.label = label\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.files)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        logscalogram = (np.load(self.root_dir + self.files[idx]+'_cqt.npy') - -24.3633)/14.2659\n",
    "        len_cqt = logscalogram.shape[1]\n",
    "        rem = len_cqt % 3\n",
    "        logscalograms = np.split(logscalogram[:, :-rem], 3, axis = 1)\n",
    "        logscalograms = np.stack(logscalograms)\n",
    "\n",
    "        weight = self.weights[idx]\n",
    "        label = (self.label[idx] >0.5).astype(int)\n",
    "        return {'logscalogram': logscalograms, 'label': label[np.newaxis, :], 'weight': weight[np.newaxis, :]}\n",
    "\n",
    "filenames = []\n",
    "root_dir = '/beegfs/bva212/openmic-2018/cqt_full/'\n",
    "\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "def my_collate(batch):\n",
    "    data = np.stack([item['logscalogram'] for item in batch])\n",
    "    target = np.concatenate([item['label'] for item in batch],axis=0)\n",
    "    weight = np.concatenate([item['weight'] for item in batch],axis=0)\n",
    "    weight_sum = weight.sum(0)\n",
    "    weight_sum = np.repeat(weight_sum[np.newaxis, :], len(batch), 0)\n",
    "    weight_sum[weight_sum==0]=1\n",
    "    weight = weight/weight_sum\n",
    "    return [torch.from_numpy(data).float(), torch.from_numpy(target).float(), torch.from_numpy(weight).float()]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class snet2_jigsaw(nn.Module):\n",
    "\n",
    "    def __init__(self, output_classes = 20):\n",
    "        '''\n",
    "        Create the 5 Conv Layer Sound Net network architecture as per the paper - https://arxiv.org/pdf/1610.09001.pdf\n",
    "        '''\n",
    "        super(snet2_jigsaw, self).__init__()\n",
    "\n",
    "        self.conv_layers = nn.Sequential(nn.Conv2d(in_channels = 1, out_channels= 16, kernel_size = 5, stride = 2, padding = 5), \n",
    "                                nn.BatchNorm2d(num_features = 16), \n",
    "                                nn.ReLU(inplace = True),\n",
    "\n",
    "                                nn.Conv2d(in_channels = 16, out_channels = 32, kernel_size = 5, stride = 2, padding = 5),\n",
    "                                nn.BatchNorm2d(32),\n",
    "                                nn.ReLU(inplace = True),\n",
    "\n",
    "                                nn.Conv2d(in_channels = 32, out_channels = 64, kernel_size = 5, padding = 5),\n",
    "                                nn.BatchNorm2d(64),\n",
    "                                nn.ReLU(inplace = True),\n",
    "                                nn.AvgPool2d(kernel_size = 3),\n",
    "\n",
    "                                nn.Conv2d(in_channels = 64, out_channels = 128, kernel_size = 4, padding = 4),\n",
    "                                nn.BatchNorm2d(128),\n",
    "                                nn.ReLU(inplace = True),\n",
    "\n",
    "                                nn.Conv2d(in_channels = 128, out_channels = 256, kernel_size = 4, padding = 4),\n",
    "                                nn.BatchNorm2d(256),\n",
    "                                nn.ReLU(inplace = True),\n",
    "                                nn.AvgPool2d(kernel_size = 3),\n",
    "\n",
    "                                nn.Conv2d(in_channels = 256, out_channels = 512, kernel_size = 3, stride = 2, padding = 3),\n",
    "                                nn.BatchNorm2d(512),\n",
    "                                nn.ReLU(inplace = True),\n",
    "\n",
    "                                nn.Conv2d(in_channels = 512, out_channels = 1024, kernel_size = 3, stride = 2, padding = 3),\n",
    "                                nn.BatchNorm2d(1024),\n",
    "                                nn.ReLU(inplace = True),\n",
    "                                nn.AdaptiveAvgPool2d(output_size = 1)\n",
    "                                )\n",
    "        self.concat_mlp_layer = nn.Sequential(nn.Linear(3072, 2048),\n",
    "                                              nn.BatchNorm1d(num_features = 2048), \n",
    "                                              nn.ReLU(inplace = True),\n",
    "                                              \n",
    "                                              nn.Linear(2048, 1024),\n",
    "                                              nn.BatchNorm1d(num_features = 1024), \n",
    "                                              nn.ReLU(inplace = True),\n",
    "                                              \n",
    "                                              nn.Linear(1024, 256),\n",
    "                                              nn.BatchNorm1d(num_features = 256), \n",
    "                                              nn.ReLU(inplace = True),\n",
    "                                             )\n",
    "        self.mlp_layer = nn.Linear(256, output_classes)\n",
    "              \n",
    "    def forward(self, input):\n",
    "        conv_strips = []\n",
    "        n_strips = input.shape[1]\n",
    "        for strip in range(n_strips):\n",
    "            conv_strip = input[:,strip]\n",
    "            conv_strip = conv_strip.unsqueeze(1)\n",
    "            conv_strips.append(self.conv_layers(conv_strip))\n",
    "\n",
    "        concat_out=torch.cat(conv_strips,1)\n",
    "        out = self.concat_mlp_layer(concat_out.view(concat_out.shape[0], -1))\n",
    "        output = self.mlp_layer(out.view(out.shape[0], -1))\n",
    "        return output\n",
    "    \n",
    "    def init_weights(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                nn.init.kaiming_normal_(m.weight, a=0, mode='fan_in', nonlinearity='relu')\n",
    "        \n",
    "# Function for testing the model\n",
    "def test_model(loader, model):\n",
    "    correct = 0\n",
    "    total_loss = 0\n",
    "    total = 0\n",
    "    total_num = 0\n",
    "    actual_arr = []\n",
    "    predicted_arr = []\n",
    "    weight_array = []\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for spectrogram, target, weight in loader:\n",
    "            spectrogram_batch, target_batch, weight_batch = spectrogram.to(device), target.to(device), weight.to(device)\n",
    "            outputs = model(spectrogram_batch)\n",
    "            loss = F.binary_cross_entropy_with_logits(outputs, target_batch,\n",
    "                                                  weight = weight_batch,\n",
    "                                                  reduction='sum')\n",
    "            predicted = (torch.sigmoid(outputs.data)>0.5).float()\n",
    "            \n",
    "            actual_arr.extend(target.view(1,-1).squeeze().numpy().astype(int).tolist())\n",
    "            predicted_arr.extend(predicted.view(1,-1).squeeze().cpu().numpy().astype(int).tolist())\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "            total += weight_batch.shape[0]\n",
    "            \n",
    "            correct += ((weight_batch != 0).float()*(predicted.eq(target_batch.view_as(predicted)).float())).sum().item()\n",
    "            total_num += (weight_batch != 0).sum().item()\n",
    "            weight_array = np.concatenate((weight_array,(weight != 0).reshape(-1).numpy().astype(int)))\n",
    "        accuracy = (100 * correct / total_num)\n",
    "        return accuracy, f1_score(actual_arr, predicted_arr, average='micro', sample_weight = weight_array), total_loss/total\n",
    "\n",
    "def train_model(train_loader, val_loader, model, optimizer, scheduler, num_epochs):\n",
    "    train_acc_list = []\n",
    "    train_loss_list = []\n",
    "    val_acc_list = []\n",
    "    val_loss_list = []\n",
    "    train_f1_list = []\n",
    "    val_f1_list = []\n",
    "    best_val_acc = 0\n",
    "    for epoch in range(num_epochs):\n",
    "        for spectrogram, target, weight in train_loader:\n",
    "            model.train()\n",
    "            spectrogram_batch, target_batch, weight_batch = spectrogram.to(device), target.to(device), weight.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(spectrogram_batch)\n",
    "            loss = F.binary_cross_entropy_with_logits(outputs, target_batch,\n",
    "                                                  weight = weight_batch,\n",
    "                                                  reduction='sum')\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        train_acc, f1_score_train, train_loss = test_model(train_loader, model)\n",
    "        val_acc, f1_score_val, val_loss = test_model(val_loader, model)\n",
    "        \n",
    "        if val_acc > best_val_acc:\n",
    "            best_val_acc = val_acc\n",
    "            best_model_state_dict = deepcopy(model.state_dict())\n",
    "        \n",
    "        train_acc_list.append(train_acc)\n",
    "        train_f1_list.append(f1_score_train)\n",
    "        train_loss_list.append(train_loss)\n",
    "        val_f1_list.append(f1_score_val)\n",
    "        val_acc_list.append(val_acc)\n",
    "        val_loss_list.append(val_loss)\n",
    "        \n",
    "        scheduler.step(val_acc)\n",
    "        print(\"Epoch:{}\".format(epoch+1))\n",
    "        print(\"Validation Accuracy:{:.2f}, Validation F1:{:.2f}, Val Loss: {:.5f}\".format(val_acc, f1_score_val, val_loss))\n",
    "        print(\"Training Acc: {:.2f}, Training F1:{:.2f}, Train Loss: {:.5f}\".format(train_acc, f1_score_train, train_loss))\n",
    "    return train_acc_list, train_loss_list, val_acc_list, val_loss_list, best_model_state_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:1\n",
      "Validation Accuracy:55.01, Validation F1:0.55, Val Loss: 0.40727\n",
      "Training Acc: 56.06, Training F1:0.56, Train Loss: 0.45947\n",
      "Epoch:2\n",
      "Validation Accuracy:55.62, Validation F1:0.56, Val Loss: 0.40822\n",
      "Training Acc: 58.52, Training F1:0.59, Train Loss: 0.43549\n",
      "Epoch:3\n",
      "Validation Accuracy:54.46, Validation F1:0.54, Val Loss: 0.41175\n",
      "Training Acc: 62.12, Training F1:0.62, Train Loss: 0.42464\n",
      "Epoch:4\n",
      "Validation Accuracy:52.40, Validation F1:0.52, Val Loss: 0.41611\n",
      "Training Acc: 59.85, Training F1:0.60, Train Loss: 0.42647\n",
      "Epoch     4: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch:5\n",
      "Validation Accuracy:52.51, Validation F1:0.53, Val Loss: 0.41553\n",
      "Training Acc: 61.93, Training F1:0.62, Train Loss: 0.41668\n",
      "Epoch:6\n",
      "Validation Accuracy:53.33, Validation F1:0.53, Val Loss: 0.41458\n",
      "Training Acc: 63.07, Training F1:0.63, Train Loss: 0.42614\n",
      "Epoch:7\n",
      "Validation Accuracy:53.44, Validation F1:0.53, Val Loss: 0.41447\n",
      "Training Acc: 63.26, Training F1:0.63, Train Loss: 0.40757\n",
      "Epoch     7: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch:8\n",
      "Validation Accuracy:52.72, Validation F1:0.53, Val Loss: 0.41666\n",
      "Training Acc: 62.31, Training F1:0.62, Train Loss: 0.42438\n",
      "Epoch:9\n",
      "Validation Accuracy:52.57, Validation F1:0.53, Val Loss: 0.41768\n",
      "Training Acc: 62.31, Training F1:0.62, Train Loss: 0.43151\n",
      "Epoch:10\n",
      "Validation Accuracy:52.30, Validation F1:0.52, Val Loss: 0.41765\n",
      "Training Acc: 61.93, Training F1:0.62, Train Loss: 0.42025\n",
      "Epoch    10: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch:11\n",
      "Validation Accuracy:53.20, Validation F1:0.53, Val Loss: 0.41581\n",
      "Training Acc: 63.64, Training F1:0.64, Train Loss: 0.41833\n",
      "Epoch:12\n",
      "Validation Accuracy:52.80, Validation F1:0.53, Val Loss: 0.41643\n",
      "Training Acc: 62.50, Training F1:0.62, Train Loss: 0.42036\n",
      "Epoch:13\n",
      "Validation Accuracy:53.51, Validation F1:0.54, Val Loss: 0.41547\n",
      "Training Acc: 63.83, Training F1:0.64, Train Loss: 0.41649\n",
      "Epoch:14\n",
      "Validation Accuracy:53.61, Validation F1:0.54, Val Loss: 0.41575\n",
      "Training Acc: 64.20, Training F1:0.64, Train Loss: 0.42806\n",
      "Epoch:15\n",
      "Validation Accuracy:52.33, Validation F1:0.52, Val Loss: 0.41819\n",
      "Training Acc: 62.31, Training F1:0.62, Train Loss: 0.41326\n",
      "Epoch:16\n",
      "Validation Accuracy:53.99, Validation F1:0.54, Val Loss: 0.41448\n",
      "Training Acc: 64.02, Training F1:0.64, Train Loss: 0.41821\n",
      "Epoch:17\n",
      "Validation Accuracy:53.12, Validation F1:0.53, Val Loss: 0.41591\n",
      "Training Acc: 64.39, Training F1:0.64, Train Loss: 0.41055\n",
      "Epoch:18\n",
      "Validation Accuracy:54.02, Validation F1:0.54, Val Loss: 0.41523\n",
      "Training Acc: 64.58, Training F1:0.65, Train Loss: 0.41211\n",
      "Epoch:19\n",
      "Validation Accuracy:53.20, Validation F1:0.53, Val Loss: 0.41726\n",
      "Training Acc: 62.88, Training F1:0.63, Train Loss: 0.42220\n",
      "Epoch:20\n",
      "Validation Accuracy:53.15, Validation F1:0.53, Val Loss: 0.41586\n",
      "Training Acc: 63.07, Training F1:0.63, Train Loss: 0.41992\n",
      "Epoch:21\n",
      "Validation Accuracy:53.12, Validation F1:0.53, Val Loss: 0.41545\n",
      "Training Acc: 63.83, Training F1:0.64, Train Loss: 0.42016\n",
      "Epoch:22\n",
      "Validation Accuracy:53.40, Validation F1:0.53, Val Loss: 0.41594\n",
      "Training Acc: 63.45, Training F1:0.63, Train Loss: 0.42610\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "sizes = [10, 50, 250, 500, 950, 'Full']\n",
    "\n",
    "Model_Data = pickle.load(open('Training_Data_Index.pkl', 'rb'))\n",
    "\n",
    "Val_dataset = ArrowOfTime(root_dir, sample_key[val_idx], Y_mask[val_idx], Y_true[val_idx])\n",
    "Val_loader = torch.utils.data.DataLoader(dataset = Val_dataset, \n",
    "                                              batch_size = BATCH_SIZE,\n",
    "                                              shuffle = False,\n",
    "                                        collate_fn = my_collate)\n",
    "\n",
    "results_dict = {}\n",
    "\n",
    "for i in range(len(sizes)):\n",
    "    if sizes[i] != 'Full':\n",
    "        sample_key_train = Model_Data[i]\n",
    "        idx_train_new = np.isin(sample_key, sample_key_train)\n",
    "        Y_mask_train = Y_mask[idx_train_new]\n",
    "        label_train = Y_true[idx_train_new]\n",
    "        Train_dataset = ArrowOfTime(root_dir, sample_key[idx_train_new], Y_mask_train, label_train)\n",
    "        Train_loader = torch.utils.data.DataLoader(dataset = Train_dataset, \n",
    "                                                      batch_size = BATCH_SIZE,\n",
    "                                                      shuffle = True,\n",
    "                                                  collate_fn = my_collate)\n",
    "    else:\n",
    "        Y_mask_train = Y_mask[train_idx]\n",
    "        label_train = Y_true[train_idx]\n",
    "        Train_dataset = ArrowOfTime(root_dir, sample_key[train_idx], Y_mask_train, label_train)\n",
    "        Train_loader = torch.utils.data.DataLoader(dataset = Train_dataset, \n",
    "                                                      batch_size = BATCH_SIZE,\n",
    "                                                      shuffle = True,\n",
    "                                                  collate_fn = my_collate)\n",
    "\n",
    "    \n",
    "    weights = torch.load('/beegfs/sc6957/capstone/models/20191106/snet2_jigsaw_large_best_model.pth')['modelStateDict']\n",
    "    model = snet2_jigsaw(2)\n",
    "    model.load_state_dict(weights)\n",
    "    model.mlp_layer = nn.Linear(256, 20)\n",
    "    model.to(device)\n",
    "    learning_rate = 10**(-5)\n",
    "    num_epochs = 50 # number epoch to train\n",
    "\n",
    "    optimizer = torch.optim.Adam([param for param in model.parameters() if param.requires_grad == True], lr=learning_rate, weight_decay = 0.01)\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='max', factor=0.1, patience=2, verbose=True, threshold=0.03,\n",
    "                                                           threshold_mode='abs', cooldown=0, min_lr=0, eps=1e-08)\n",
    "\n",
    "    train_acc_list, train_loss_list, val_acc_list, val_loss_list, best_model_state_dict = train_model(Train_loader, Val_loader, model, optimizer, scheduler, num_epochs)\n",
    "    results_dict[sizes[i]] = {\n",
    "        'train_acc_list':train_acc_list,\n",
    "        'train_loss_list': train_loss_list,\n",
    "        'val_acc_list': val_acc_list,\n",
    "        'val_loss_list': val_loss_list,\n",
    "        'best_model_state_dict': best_model_state_dict\n",
    "    }\n",
    "pickle.dump(results_dict, open('/scratch/rc3620/Capstone/Size_Exp_Jigsaw_Wt_Results.pkl'.format(sizes[i]),'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "results_dict = {}\n",
    "model1 = snet2_jigsaw(20).to(device)\n",
    "model = snet2_jigsaw(20).to(device)\n",
    "\n",
    "for i in range(len(sizes)):\n",
    "    if sizes[i] != 'Full':\n",
    "        sample_key_train = Model_Data[i]\n",
    "        idx_train_new = np.isin(sample_key, sample_key_train)\n",
    "        Y_mask_train = Y_mask[idx_train_new]\n",
    "        label_train = Y_true[idx_train_new]\n",
    "        Train_dataset = ArrowOfTime(root_dir, sample_key[idx_train_new], Y_mask_train, label_train)\n",
    "        Train_loader = torch.utils.data.DataLoader(dataset = Train_dataset, \n",
    "                                                      batch_size = BATCH_SIZE,\n",
    "                                                      shuffle = True,\n",
    "                                                  collate_fn = my_collate)\n",
    "    else:\n",
    "        Y_mask_train = Y_mask[train_idx]\n",
    "        label_train = Y_true[train_idx]\n",
    "        Train_dataset = ArrowOfTime(root_dir, sample_key[train_idx], Y_mask_train, label_train)\n",
    "        Train_loader = torch.utils.data.DataLoader(dataset = Train_dataset, \n",
    "                                                      batch_size = BATCH_SIZE,\n",
    "                                                      shuffle = True,\n",
    "                                                  collate_fn = my_collate)\n",
    "\n",
    "    \n",
    "    model.load_state_dict(model1.state_dict())\n",
    "    learning_rate = 10**(-5)\n",
    "    num_epochs = 50 # number epoch to train\n",
    "\n",
    "    optimizer = torch.optim.Adam([param for param in model.parameters() if param.requires_grad == True], lr=learning_rate, weight_decay = 0.01)\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='max', factor=0.1, patience=2, verbose=True, threshold=0.03,\n",
    "                                                           threshold_mode='abs', cooldown=0, min_lr=0, eps=1e-08)\n",
    "\n",
    "    train_acc_list, train_loss_list, val_acc_list, val_loss_list, best_model_state_dict = train_model(Train_loader, Val_loader, model, optimizer, scheduler, num_epochs)\n",
    "    results_dict[sizes[i]] = {\n",
    "        'train_acc_list':train_acc_list,\n",
    "        'train_loss_list': train_loss_list,\n",
    "        'val_acc_list': val_acc_list,\n",
    "        'val_loss_list': val_loss_list,\n",
    "        'best_model_state_dict': best_model_state_dict\n",
    "    }\n",
    "    pickle.dump(results_dict, open('/scratch/rc3620/Capstone/Size_Exp_Random_Wt_Results_size_{}.pkl'.format(sizes[i]),'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean = 0.\n",
    "std = 0.\n",
    "for images, _, _ in Train_loader:\n",
    "    batch_samples = images.size(0) # batch size (the last batch can have smaller size!)\n",
    "    images1 = images.reshape(batch_samples, images.size(1), -1)\n",
    "    mean += images1.mean(2).sum(0)\n",
    "    std += images1.std(2).sum(0)\n",
    "    \n",
    "mean /= len(Train_loader.dataset)\n",
    "std /= len(Train_loader.dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-24.3633]) tensor([14.2659])\n"
     ]
    }
   ],
   "source": [
    "print(mean, std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "model = conv_net().to(device)\n",
    "model.load_state_dict(reqd_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(results_dict, open('Size_Exp_Jigsaw_Wt_Results.pkl'.format(sizes[i]),'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys([10, 50, 250, 500])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_dict.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision.models as models\n",
    "resnet18 = models.resnet18().to(device)\n",
    "alexnet = models.alexnet().to(device)\n",
    "vgg16 = models.vgg16().to(device)\n",
    "squeezenet = models.squeezenet1_0().to(device)\n",
    "densenet = models.densenet161().to(device)\n",
    "inception = models.inception_v3().to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "results_dict = pickle.load(open('Size_Exp_Jigsaw_Wt_Results.pkl','rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[53.35987453251297,\n",
       " 56.55688261551454,\n",
       " 57.53408131258294,\n",
       " 57.63059476414525,\n",
       " 58.6681143684401,\n",
       " 59.6453130655085,\n",
       " 60.35709977078055,\n",
       " 59.40402943660273,\n",
       " 61.80480154421522,\n",
       " 62.142598624683316,\n",
       " 60.091687778984195,\n",
       " 61.81686572566051,\n",
       " 60.28471468210882,\n",
       " 63.56617203522741,\n",
       " 63.771263119797325,\n",
       " 63.67474966823501,\n",
       " 63.771263119797325,\n",
       " 63.69887803112559,\n",
       " 63.98841838581252,\n",
       " 63.96429002292194,\n",
       " 63.952225841476654,\n",
       " 64.0125467487031,\n",
       " 63.928097478586075,\n",
       " 63.89190493425021,\n",
       " 63.928097478586075,\n",
       " 63.940161660031364,\n",
       " 63.9039691156955,\n",
       " 64.0004825672578,\n",
       " 63.819519845578476,\n",
       " 63.928097478586075,\n",
       " 63.96429002292194,\n",
       " 63.843648208469055,\n",
       " 63.89190493425021,\n",
       " 63.72300639401617,\n",
       " 63.819519845578476,\n",
       " 63.759198938352036,\n",
       " 64.0125467487031,\n",
       " 63.940161660031364,\n",
       " 64.02461093014838,\n",
       " 64.02461093014838,\n",
       " 63.952225841476654,\n",
       " 63.80745566413319,\n",
       " 63.7953914826879,\n",
       " 63.9039691156955,\n",
       " 63.928097478586075,\n",
       " 63.6868138496803,\n",
       " 63.843648208469055,\n",
       " 63.819519845578476,\n",
       " 63.9039691156955,\n",
       " 63.9039691156955]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_dict[1000]['val_acc_list']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
